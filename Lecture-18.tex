% -----------------------------*- LaTeX -*------------------------------
\documentclass[12pt]{report}
\input{header}
\usepackage{scribe_e1244}
\usepackage{times}
\usepackage{bm}
\begin{document}

\lecturer{Aditya Gopalan, Parimal Parag}		% optional, put lecturer's name here
\scribe{Preeti Sahu \& Prabhasa K}					% required, put your name here
\lecturenumber{18}						% required, must be a number
\lecturedate{March 9}					% required, omit year
\maketitle

% ----------------------------------------------------------------------
%opening

\begin{center}
{\Large \bf Continuation of Bayes Estimators and Performance Analysis}
\end{center}

\section{Introduction}

In this lecture, we will look at more examples of the Bayes Estimator with a different prior distribution on $\theta$. We will learn performance analysis of these estimators, and a method to evaluate the best unbiased estimator through the Cramer-Rao Lower Bound.

\subsection{Recap - Bayes Estimator}
\par Given $\{f(x|\theta):\theta \in \Theta\}$, the underlying assumption is that $\theta \in \Theta$ is randomly chosen from a prior distribution $\pi$ over $\Theta$, and therefore $X_1,X_2,...,X_n$ are i.i.d over distribution $f(\textbf{x}|\theta)$. If $x_1,x_2,\dots,x_n$ are the observed samples, construct the posterior distribution for $\theta \in \Theta$ using Bayes rule.
\begin{align}
\mathbb{P}(\theta|(X_1=x_1,X_2=x_2,\dots,X_n=x_n))
&=\frac{\mathbb{P}(\theta)\mathbb{P}(X_1=x_1,X_2=x_2,\dots,X_n=x_n|\theta)}{\int\limits_0^1 (\mathbb{P}(X_1=x_1,X_2=x_2,\dots,X_n=x_n\vert\theta')P(\theta')} \nonumber\\
&=\displaystyle{\mathrm{Beta}\left(1+\sum\limits_i {x_i},1+n-\sum\limits_i x_i\right)}
\end{align}
One can write down possible estimators depending on the posterior distribution.

In the last lecture, we concluded with an example of a Bernoulli Bayes Estimator with uniform prior distribution on $\theta$. Upon evaluating the posterior distribution, the possible estimators are:\\

Expectation\,\,\,=\,\,\,$\displaystyle{\frac{1+\sum\limits_i {x_i}}{n+2}}$, where $n$ is the number of observations.\\

\begin{note}
 Choice of prior is subjective, i.e. up to the designer.
\end{note}

\vspace{2cm}

\section{Bayes Estimators}
\begin{exmp}\textbf{(Bernoulli Bayes Estimators)} Let $X_1,X_2,\dots,X_n$ be i.i.d Bernoulli($\theta$), $\theta \in [0,1]$. Let the prior be a $\mathrm{Beta}(a,b)$ distribution, that is:
\begin{align}
\pi(\theta) = 
\begin{cases}
 \displaystyle{\frac{\theta^{a-1}(1-\theta)^{b-1}}{\beta(a,b)}},       \,\,\,\,\,\,\,\mbox{if}\,\,\,\theta \in [0,1] \\
0,\hspace{85pt}\mbox{otherwise}
\end{cases}
\end{align} 
where,
\begin{align}
\beta(a,b) = \displaystyle{\int\limits_0^1 x^{a-1}(1-x)^{b-1}dx}.
\end{align}
Consider the posterior distribution,
\begin{align}
\pi(\theta|(x_1,x_2,\dots,x_n))&=\displaystyle{\frac{\pi(\theta)f(x_1,x_2,\dots,x_n|\theta)}{f(x_1,x_2,\dots,x_n)}},\\
&=\displaystyle{ \frac{1}{f(x)} \frac{\theta^{a-1}(1-\theta)^{b-1}}{\beta(a,b)} \prod\limits_{i=1}^n{\theta^{x_i}(1-\theta)^{1-x_i}}},\nonumber\\
&=\displaystyle{\frac{\theta^{\sum\limits_i {x_i}+a-1}(1-\theta)^{n-\sum\limits_i x_i +b-1}}{\beta(a,b)f(x)}},\nonumber
\end{align}
which is also a $\mathrm{Beta}\left(\sum\limits_i {x_i}+a,n-\sum\limits_i x_i +b\right)$ distribution.\\
Given the posterior distribution, the possible estimator is Expectation\,\,\,=\,\,\,$\displaystyle{\frac{\sum\limits_i {x_i}+a}{a+b+n}}$
%\item Median
\end{exmp}
\begin{defn}{Let $F:= \{f(x|\theta):\theta \in \Theta\}$, then a class $\prod$ of prior distributions on $\Theta$ is a \textit{conjugate family} for $F$, if the posterior distribution is in the class $\prod \forall f \in F$ and for all priors in $\prod$.}
\end{defn}


\begin{exmp}(\textbf{Normal Bayes Estimators})
Let $X_1$, $X_2$,...$X_n$ be i.i.d $\mathcal{N}(\theta , 1)$, and suppose that the prior distribution on $\theta$ is $\mathcal{N}(\mu , \tau ^2)$, (Here, we assume that both $\mu$ and $\tau ^2$ are known). The posterior distribution of $\theta$ is also normal with mean and variance given by 
\begin{align*}
\mathbb{E}(\theta | x) &= \frac{\tau ^2}{\frac{1}{n} + \tau ^2} \bar{x} + \frac{\frac{1}{n}}{\frac{1}{n} + \tau ^2} \mu \end{align*}
\begin{align*}
Var(\theta | x) &= \frac{\frac{1}{n} \tau ^2}{\frac{1}{n} + \tau ^2}\\
\end{align*}
where $\bar{x} = \frac{1}{n} \sum_{i=1}^{n}(x_i)$, and $n$ is the number of observations. Note that the posterior distribution belongs to the same class of distribution as that of prior distribution. Hence, Normal distribution is its own conjugate family.
\end{exmp}
\section{Performance Analysis of Estimators}
So far, we have studied three types of estimators. In this section, we will study the performance of these estimators. Although there are several criteria for evaluating the performance, here we will focus our attention to distance functions. In other words, the mean squared error (MSE) criterion.
\subsection{Mean Squared Error Criterion}
\begin{defn}
The MSE of a point estimator $W$ of a parameter $\theta$ is a function of $\theta$ denoted by $MSE(W ; \theta)$ and defined by
\begin{align*}
MSE(W ; \theta) &= \mathbb{E}_\theta[(W - \theta)^2]
\end{align*}
Specifying the dependence of $W$ on $X$ expilicitly, it turns out that
\begin{align*}
MSE(W ; \theta) &= \mathbb{E}_\theta[(W(X) - \theta)^2]
\end{align*}
Applying the definition of expectation:
\begin{align*}
MSE(W ; \theta) &= \int_{\mathbb{R}^n} (W(x) - \theta)^2 f(x | \theta) dx
\end{align*} 
\end{defn}
\subsection{Representation of MSE as Variance \& Bias}
The variance of a point estimator $W$ of a parameter $\theta$:
\begin{align*}
Var_\theta[W]  = \mathbb{E}_\theta[(W - \mathbb{E}_\theta(W))^2]
\end{align*} 
Bias of a point estimator $W$ of a parameter $\theta$: 
\begin{align*}
Bias_\theta[W] = \mathbb{E}_\theta[W] - \theta
\end{align*}
By the definition of MSE,
\begin{align*}
MSE(W ; \theta) &= \mathbb{E}_\theta[(W - \theta)^2]\\
&= \mathbb{E}_\theta[(W - \mathbb{E}_\theta(W) + \mathbb{E}_\theta(W) - \theta)^2]\\
&= \mathbb{E}_\theta[(W - \mathbb{E}_\theta(W))^2] + \mathbb{E}_\theta[(\mathbb{E}_\theta(W) - \theta)^2] + 2\mathbb{E}_\theta[(W - \mathbb{E}_\theta(W)) \times (\mathbb{E}_\theta(W) - \theta)] \\
&= \mathbb{E}_\theta[(W - \mathbb{E}_\theta(W))^2] + (\mathbb{E}_\theta(W) - \theta)^2\\
&= Var_\theta[W] + (Bias_\theta[W])^2
\end{align*} 
In the second equality, we have added and subtracted $\mathbb{E}_\theta[W]$. The fourth equality follows from the fact that its second term is a constant value and the third term is equal to zero. The last equality follows from the definition of variance and bias of an estimator.
\begin{note}
For unbiased estimators, the mean of the estimator is equal to the parameter $\theta$. Thus, the bias is zero, so $\bm{MSE[W;\theta] = Var_\theta[W]}$.
\end{note}

\begin{exmp}(\textbf{Normal MSE})
Let $X_1$, $X_2$, ....,$X_n$ be i.i.d samples from $\mathcal{N}(\mu , \sigma ^2)$. We will discuss two kinds of estimators:
\\
\\ 1. \textbf{ Estimator:}
 \begin{align*}
      \bar{X} &= \frac{1}{n} \sum_{i=1}^{n}(X_i) = \mu &
      S^2 &= \frac{1}{n-1} \sum_{i=1}^{n}(X_i - \bar{X})^2
 \end{align*}
 
\begin{align*}
\mathbb{E}[\bar{X}] = \mu \ , \ \mathbb{E}[S^2] = \sigma ^2 \ for \ all \ \mu \ and \  \sigma ^2
\end{align*}
\begin{align*}
MSE(\bar{X} ; (\mu , \sigma ^2)) &= Var_{\mu , \sigma ^2}(\bar{X}) = \mathbb{E}_{\mu , \sigma ^ 2}[(\bar{X} - \mu)^2] = \frac{\sigma ^2}{n}\\
MSE(S^2 ; (\mu , \sigma ^2)) &= Var_{\mu , \sigma ^2}(S^2) = \mathbb{E}_{\mu , \sigma ^ 2}[(S^2 - \sigma ^2)^2] = \frac{2 \sigma ^4}{n - 1}
\end{align*}
2. \textbf{Alternative Estimator:}
\begin{align*}
\bar{X} &= \frac{1}{n} \sum_{i=1}^{n}(X_i) = \mu &
\hat{\sigma}^2 &= \frac{1}{n} \sum_{i=1}^{n}(X_i - \bar{X})^2 = \frac{n-1}{n} S^2
\end{align*}
\begin{align*}
\mathbb{E}[\bar{X}] = \mu \ , \ \mathbb{E}[\hat{\sigma}^2] &= \mathbb{E}[\frac{n-1}{n} S^2] = \frac{n-1}{n} \sigma ^2
\end{align*}
\begin{align*}
Var[\hat{\sigma}^2] &= Var[\frac{n-1}{n} S^2] = \frac{2(n-1)}{n^2} \sigma ^4\\
Bias[\hat{\sigma}^2] &= (\mathbb{E}[\hat{\sigma}^2] - \sigma ^2)^2 =     \Big( \frac{n-1}{n} \sigma ^2 - \sigma ^2 \Big)^2 = \frac{\sigma ^4}{n^2}
\end{align*}
\begin{align*}
MSE(\hat{\sigma}^2 ; (\mu , \sigma ^2)) = \frac{2(n-1)}{n^2} \sigma ^4 + \frac{\sigma ^4}{n^2} = \frac{(2n-1)}{n^2} \sigma ^4 < \frac{2 \sigma ^4}{n-1} = MSE(S^2 ; (\mu , \sigma ^2))
\end{align*}
Which shows that $\hat{\sigma}^2$ has smaller MSE than $S^2$.

\end{exmp}

\vspace{2cm}

\section{Best Unbiased Estimators (BUE)}
In the previous section, we realized that the MSE depends on the choice of the estimator $\hat{\sigma}^2$. Hence, there is no "best Biased MSE Estimator". So we shall limit ourselves to unbiased estimators.

%we saw that there is no one "best MSE" estimator. The reason is that the class of all estimators is too large a class. One way of finding a "best estimator" is to limit the class of estimators. So here we will consider the class of unbiased estimators only.

\begin{defn}
An estimator $W^*$ is a best unbiased estimator of $g(\theta)$ if it satisfies 
\begin{enumerate}
\item $\bm{\mathbb{E}_\theta[W^*] = g(\theta)}, \forall \theta \in \Theta$ and,
\item For any other estimator $W$ with $\mathbb{E}_\theta[W] = g(\theta), \bm{Var_\theta(W^*) \leq Var_\theta(W)} \ \forall  \theta  \in  \Theta$
\end{enumerate}
\begin{note}
$W^*$ is also called a uniform minimum variance unbiased estimator of $g(\theta)$. Finding this uniform estimator $\forall \theta$ is difficult, so we attain a lower bound on the variance.
\end{note}
\end{defn}

\begin{thm}(\textbf{Cramer - Rao Lower Bound})
Let $X_1$, $X_2$, ....,$X_n$ be i.i.d samples with pdf $f(x | \theta)$ and let $W(X) = W(X_1, X_2,....,X_n)$ be any estimator satisfying
\begin{enumerate}
\item $\frac{d}{d\theta}\mathbb{E}_\theta[W(X)] = \int_{\mathbb{R}^n} \frac{\partial}{\partial \theta}[W(x) f(x | \theta)] dx$ and 
\item $Var_\theta(W(X)) < \infty$ 
\end{enumerate}
Then 
\begin{align*}
Var_\theta(W(X)) \geq \frac{\Big(\frac{d}{d\theta}\mathbb{E}_\theta[W(X)] \Big)^2}{\mathbb{E}_\theta \Big(\big(\frac{\partial}{\partial \theta} \log f(X | \theta)\big)^2\Big)}
\end{align*}
\end{thm}
\begin{proof}
From the first condition, we have
\begin{align}
\frac{d}{d\theta}\mathbb{E}_\theta[W(X)] &= \int_{\mathbb{R}^n} W(x) \Big[\frac{\partial}{\partial \theta}f(x | \theta)\Big] dx \nonumber \\
&= \mathbb{E}_\theta \Big[ W(X) \frac{\frac{\partial}{\partial \theta}f(X | \theta)}{f(X | \theta)} \Big] \nonumber \\
&= \mathbb{E}_\theta \Big[ W(X) \frac{\partial}{\partial \theta}\log f(X | \theta) \Big]
\end{align}

The second equality is got by multiplying $\frac{f(X|\theta)}{f(X|\theta)}$ on the RHS, and then using the definition of expectation. %The third equality uses the property of the derivative of Logarithmic function.

Substituting $W(X) = 1$ in the above equation, we get
\begin{align}
\mathbb{E}_\theta \Big[\frac{\partial}{\partial \theta}\log f(X | \theta) \Big] = \frac{d}{d\theta} \mathbb{E}_\theta[1] = 0
\end{align}

We shall now cleverly use the Cauchy - Schwartz inequality to prove the CRLB as follows
\begin{align*}
\sum_{i=1}^{n} a_i b_i \leq \sqrt{\sum_{i=1}^{n} a_i^2}\sqrt{\sum_{i=1}^{n} b_i^2}
\end{align*}
For two random variables $A$ and $B$,
\begin{align*}
\mathbb{E}(AB) \leq \sqrt{\mathbb{E}(A^2)}\sqrt{\mathbb{E}(B^2)}
\end{align*} 
Substitute 
\begin{align*}
A := U - \mathbb{E}(U)\\
B := V - \mathbb{E}(V)
\end{align*}
For some random variables $U$ and $V$\\
It turns out that 
\begin{align}
COV(U,V) & \leq \sqrt{Var(U)} \sqrt{Var(V)} \nonumber \\
[COV(U,V)]^2 & \leq Var(U) \  Var(V) \nonumber\\
Var(U) & \geq \frac{[COV(U,V)]^2}{Var(V)}
\end{align}
Now choose 
\begin{align*}
U &= W(X) \  and\\
V &= \frac{\partial}{\partial \theta} \log f(X | \theta)
\end{align*}
Therefore,
$\mathbb{E}_\theta \Big[ W(X) \frac{\partial}{\partial \theta}\log f(X | \theta) \Big]$ represents covariance between $W(X)$ and $\frac{\partial}{\partial \theta}\log f(X | \theta)$, that is, from (18.5)
\begin{align}
COV_\theta \Big[ W(X), \frac{\partial}{\partial \theta}\log f(X | \theta) \Big] = \mathbb{E}_\theta \Big[ W(X) \frac{\partial}{\partial \theta}\log f(X | \theta) \Big] = \frac{d}{d\theta}\mathbb{E}_\theta[W(X)]
\end{align}
Now from (18.6),
\begin{align}
Var_\theta \Big[\frac{\partial}{\partial \theta}\log f(X | \theta) \Big] = \mathbb{E}_\theta \Big(\frac{\partial}{\partial \theta}\log f(X | \theta) \Big)^2
\end{align}
Substituting  (18.9) and (18.10) in (18.8), we obtain
\begin{align*}
Var_\theta(W(X)) \geq \frac{\Big(\frac{d}{d\theta}\mathbb{E}_\theta[W(X)] \Big)^2}{\mathbb{E}_\theta \Big(\big(\frac{\partial}{\partial \theta} \log f(X | \theta)\big)^2\Big)}
\end{align*}
This completes the proof.
\end{proof}
   
\end{document}