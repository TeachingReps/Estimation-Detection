\documentclass[11pt,english]{article}
\input{header}
\usepackage{algorithm,algpseudocode}
\title{LECTURE 25 : LEVINSON FILTERING}


\begin{document}
\maketitle
\noindent 
Levinson filtering is concerned with one-step prediction of a random sequence whose second order statistics are stationary in time. The goal is to find the best linear estimator of ${Y_{t+1}}$ given the observations $\{{Y_0}$,${Y_1}$,...,${Y_t}\}$. 
\begin{defn}
Observation sequence $\{Y_n\}$ ,where $n \in (-\infty,\infty)$ is ``wide sense stationary" process (w.s.s), if the mean $\mu=\mathbb{E}\{Y_n\}$ is constant, and the covariance function,
\begin{equation*}
C_y(\tau)=C_y(t,t+\tau) = \mathbb{E}\{Y_tY_{t+\tau}\},~\forall~\tau \in \mathbf{Z},
\end{equation*}
depends only on the time difference $\tau$.
\end{defn}
Let, $X_t = Y_{t + 1}$ (refer lecture 24 for the notation), the cross-covariance function of $X_n=Y_{n+1}$ and $Y_n$, where $n \in (-\infty,\infty)$ is,
\begin{align*}
C_{X,Y}(t,l) &= Cov(X_t,Y_l) \\
& =  Cov(Y_{t+1},Y_l) \\
& = C_y(t+1-l)
\end{align*}
The Weiner-Hopf equation i.e {$\sigma_{XY} = \Sigma_y h_t$ becomes,
\begin{equation}\label{eqn:wh1}
\begin{bmatrix}
C_y(t+1)\\
\vdots\\
C_y(1)
\end{bmatrix}
=
\begin{bmatrix}
C_y(0) & \ldots & C_y(t)\\
\vdots & \ddots & \vdots\\
C_y(t) & \ldots & C_y(0)
\end{bmatrix}
\quad
\begin{bmatrix}
h_{t,0}\\
\vdots\\
h_{t,t}
\end{bmatrix},
\end{equation}
a set of equations also known as \emph{Yule Walker equations}.
\begin{equation*}
\mathbf{\Sigma}_{Y}^{0:t} =
\begin{bmatrix}
C_y(0) & \ldots & C_y(t)\\
\vdots & \ddots & \vdots\\
C_y(t) & \ldots & C_y(0)
\end{bmatrix},
\end{equation*}
where $\Sigma_{Y}^{0:t}$ is a Toeplitz matrix , meaning its enries are constant along the diagonals. Also, a $t\times t$ Toeplitz matrix can be inverted in number of operations that is of the order of $t^2$, as against $t^3$ for a unstructured matrix.
\subsection{Levinson-Durbin algorithm}
In this section, we discuss the algorithm to compute $h_{t,0},\ldots,h_{t,t}$ in number of operations of the order of $t^2$. Let  $\hat{Y}_{t+1}=-\sum_{n=0}^t a_{{t+1},{t+1-n}}Y_n$, then Yule-Walker equations become,
\begin{equation}
-\begin{bmatrix}
C_y(t+1)\\
\vdots\\
C_y(1)
\end{bmatrix}
= \begin{bmatrix}
C_y(0) & \ldots & C_y(t)\\
\vdots & \ddots & \vdots\\
C_y(t) & \ldots & C_y(0)
\end{bmatrix}\quad 
\begin{bmatrix}
a_{t+1,t+1}\\
\vdots\\
a_{t+1,1}
\end{bmatrix}
\end{equation}
Let $e_{t+1} := \hat{Y}_{t+1}-Y_{t+1}$, and let $\epsilon_{t+1}= \mathbb{E}\{e_{t+1}^2\}$,
\begin{align*} 
\epsilon_{t+1} &= \mathbb{E}\{e_{t+1}(-\sum_{n=0}^t a_{t+1,t+1-n}Y_n-Y_{t+1})\},\\
&= \mathbb{E}\{-e_{t+1}Y_{t+1}\},\\
&= -\mathbb{E}\{(-\sum_{n=0}^t a_{t+1,t+1-n}Y_n-Y_{t+1})Y_{t+1}\},\\
&= C_Y(0)+\sum_{n=0}^t C_y(t+1-n)a_{t+1,t+1-n}.\\
\end{align*}
Now transform eq. \eqref{eqn:wh1} by, (a) making LHS=$\vec{0}$,
\begin{equation}
\begin{bmatrix}
0\\
\vdots\\
0 
\end{bmatrix} = 
\begin{bmatrix}
C_Y(0) &\ldots &C_Y(t) &C_Y(t+1)\\
\vdots &\ddots &\vdots &\vdots\\
c_Y(t) &\ldots &C_Y(0) &C_Y(1)

\end{bmatrix}
\begin{bmatrix}
a_{t+1,t+1}\\
\vdots\\
a_{t+1,1}\\
1
\end{bmatrix},
\end{equation}
and (b) add equation for $\epsilon_{t+1}$,
\begin{equation}\label{eqn:eq0}
\begin{bmatrix}
0\\
\vdots\\
0 \\
\epsilon_{t+1}
\end{bmatrix} = 
\begin{bmatrix}
C_Y(0) &\ldots &C_Y(t+1)\\
\vdots &\ddots &\vdots\\
c_Y(t) &\ldots &C_Y(1)\\
C_Y(t+1) &\ldots &\ldots C_y(0)

\end{bmatrix}
\begin{bmatrix}
a_{t+1,t+1}\\
\vdots\\
a_{t+1,1}\\
1\\
\end{bmatrix}.
\end{equation}
Now, consider normal equations for prediction at order $(t+1)$. Let's add an extra equation
\begin{equation}\label{eqn:eq1}
\begin{bmatrix}
\alpha_{t+1}\\
0\\
\vdots\\
0 \\
\epsilon_{t+1}
\end{bmatrix} = 
\begin{bmatrix}
C_Y(0) &\ldots &C_Y(t+1) &C_y(t+2)\\
C_Y(1) &C_Y(0) &\ldots &C_Y(t+1)\\
\vdots &\vdots &\ddots &\vdots\\
C_Y(t+1) &C_Y(t+1) &\ldots &C_Y(0)


\end{bmatrix}
\begin{bmatrix}
0\\
a_{t+1,t+1}\\
\vdots\\
a_{t+1,1}\\
1\\
\end{bmatrix},
\end{equation}
where $\alpha_{t+1} := C_y(1)a_{t+1,t+1} + \ldots + C_y(t+1)a_{t+1,1} + C_y(t+2)$. Transforming eqn. \eqref{eqn:eq1} by reversing rows and then columns,
\begin{equation}\label{eqn:eq2}
\begin{bmatrix}
\epsilon_{t+1}\\
0\\
\vdots\\
0 \\
\alpha_{t+1}
\end{bmatrix} = 
\begin{bmatrix}
C_Y(0) &\ldots &C_Y(t+1) &C_y(t+2)\\
C_Y(1) &C_Y(0) &\ldots &C_Y(t+1)\\
\vdots &\vdots &\ddots &\vdots\\
C_Y(t+2) &C_Y(t+1) &\ldots &C_Y(0)
\end{bmatrix}
\begin{bmatrix}
1\\
a_{t+1,1}\\
\vdots\\
a_{t+1,t+1}\\
0\\
\end{bmatrix}
\end{equation}\\
Now multiplying eqn. \eqref{eqn:eq2} with $K_{t+1}$ and then adding with eqn. \eqref{eqn:eq1}, where $K_{t+1} = -\frac{\alpha_{t+1}}{\epsilon_{t+1}}$, we get
\begin{equation}\label{eqn:eq3}
\begin{bmatrix}
0 \\
\vdots\\
\vdots\\
0\\
\epsilon_{t+1}-\frac{\alpha_{t+1}^2}{\epsilon_{t+1}}
\end{bmatrix} = 
\begin{bmatrix}
C_Y(0) &\ldots &C_Y(t+1) &C_y(t+2)\\
C_Y(1) &C_Y(0) &\ldots &C_Y(t+1)\\
\vdots &\vdots &\ddots &\vdots\\
C_Y(t+2) &C_Y(t+1) &\ldots &C_Y(0)
\end{bmatrix}\quad 
\begin{bmatrix}
b_{t+2,t+2}\\
\vdots\\
\vdots\\
b_{t+2,1}
\end{bmatrix}
\end{equation}\\
Comparing the form of eqn \eqref{eqn:eq3} and \eqref{eqn:eq0}, by inspection, we get,
\begin{equation}
\epsilon_{t+2} = \epsilon_{t+1} - \frac{\alpha_{t+1}^2}{\epsilon_{t+1}} = \epsilon_{t+1} - \frac{(K_{t+1}\epsilon_{t+1})^2}{\epsilon_{t+1}} = \epsilon_{t+1}(1-K_{t+1}^2).
\end{equation}
For order of $t$ operations, we have, $a_{t+2,t+2} = K_{t+1}$, and 
$a_{t+2,t+2-n} = a_{t+1,t+2-n} + K_{t+1}a_{t+1-n}~1\leq n \leq t+1$.

\begin{exmp} Illustration\\
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Time&0&1&2\\
Observation&$\phi$&$Y_0$&$Y_0,Y_1$\\
To Predict&$Y_0$&$Y_1$&$Y_2$\\
\hline
Operations&&&\\ 
a&$\hat{Y_0}=0$&$a_{1,1} = K_0$&$a_{2,2} = K_1$\\
b& $\epsilon_{0}=C_y(0)$&$\epsilon_1 = (1-K_0^2)\epsilon_0$&$a_{2,1} = a_{1,1} + k_1a_{1,1}$\\
c& $k_0=-\frac{C_y(1)}{C_y(0)}$&$k_1 = -\frac{[C_Y(2) + C_Y(1)a_{1,1}]}{\epsilon_1}$&$\epsilon_2 = (1-K_1^2)\epsilon_1$\\
d&&&$k_2 = -\frac{[C_Y(3) + C_Y(2)a_{2,1} + C_Y(1)a_{2,2}]}{\epsilon_2}$\\
\hline
\end{tabular}
\end{center}
\end{exmp}
\subsection{Pseudocode: Levinson-Durbin}
\begin{center}
\begin{algorithmic}[1]
\State Initialize $\epsilon_{0} = C_y(0)$ and $K_0 = -\frac{C_Y(1)}{C_Y(0)}$
\For{$t=1,2,\dots$ } 
\State $a_{t,t} = K_{t-1}$
\State $a_{t,t-n} = a_{t-1,t-n} + K_{t-1}a_{t-1-n}$, where $1~\leq n \leq t-1$
\State $\epsilon_t = \epsilon_{t-1}(1-k_{t-1}^2)$
\State $k_t=-\frac{C_y(t+1) + \sum_{n=0}^{t-1}C_y(n+1)a_{t,t-n}}{\epsilon_t}$
\EndFor
\end{algorithmic}
\end{center}

\begin{rem}{Interpretation of $K_t$:}\\
By Orthogonality condition 'error and observation are orthogonal', i.e.,
$(Y_t-\hat{Y_t})\quad \coprod \quad (Y_0,Y_1,....Y_{t-1})$. But the error $Y_t - \hat{Y_t}$ need not be orthogonal to $Y_{-1}$, and hence,
\begin{align*}
E[(Y_t - \hat{Y_t})Y_{-1}] &= E[(Y_t + \sum_{n=0}^{t-1}a_{t,t-n}Y_n)Y_{-1}],\\
&= C_Y(t+1) + \sum_{n=0}^{t-1}a_{t,t-n}C_Y(n+1),\\
&= \alpha_t.
\end{align*}
Hence,
\begin{align*}
K_t &= -\frac{\alpha_t}{\epsilon_t},\\
&= \frac{E[(\hat{Y_t}-Y_t)Y_{-1}]}{E[(\hat{Y_t}-Y_t)^2]},\\
&= \frac{Cov[\hat{Y_t}-Y_t,Y_{-1}]}{E[(\hat{Y_t}-Y_t)^2]},
\end{align*}
where $K_t$ are called PARCOR (Partial Correlation) coefficients.
\end{rem}
\begin{rem}{Mean square error (MSE) at time $t$ is monotone, }
$\epsilon_{0} \geq \epsilon_{1} \geq \epsilon_{2} \geq \dots$
\end{rem}
\begin{rem}
Suppose for all $m>M$, we have $\epsilon_m=\epsilon_{m+1}=\dots$. This is called "stalling". When stalling occurs, the prediction errors become orthogonal.
\end{rem}
%Where 
%\begin{equation*}
%\mathbf{M} = 
%\begin{bmatrix}
%C_Y(0) &\ldots &C_Y(t) &C_Y(t+1)\\
%\vdots &\ddots &\vdots &\vdots\\
%C_Y(t) &\ldots &C_Y(0) &C_Y(1)
%\end{bmatrix}
%\end{equation*}\\
\end{document}