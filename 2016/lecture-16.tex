\documentclass[a4paper,english,12pt]{article}
\input{header}
\usepackage{accents}

\newcommand{\se}{\mathbb{E}}
\newcommand{\rr}{\mathbb{R}}
\newcommand{\spc}{\mathbb{P}}

\DeclareMathOperator*{\mn}{min}
\DeclareMathOperator*{\amn}{argmin}
\newcommand{\ubar}[1]{\underaccent{\bar}{#1}}
\newcommand{\Tau}{\mathrm{T}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
%opening
%\title{Lecture 15: Sequential Detection}
%\author{}

\begin{document}
		\begin{center}
			\Large{Lecture 16}\\
			\vspace{12pt}
			{\huge{Sequential Detection}}\\
				\vspace{15pt}
					March 03, 2016
		\end{center}
		\vspace{20pt}
		
	%	\maketitle
\section{Introduction}
In the last lecture, we introduced the idea of sequential detection. A sequential detector uses a random number of samples based on the observation sequence to achieve a desired level of performance. This is in contrast with fixed-sample-size detectors, which optimize the performance given a fixed number of samples.
\par Recall that a sequential decision rule is the pair  $(\underline{\phi},\underline{\delta})$, with
\begin{align}
\phi_{j}(Y_{1},\ldots,Y_{j})&\in\{0,1\},\nonumber\\
\text{and}~~\delta_{j}(Y_{1},\ldots,Y_{j})&\in\{H_{0},H_{1}\}.
\end{align}
To find the optimal \textit{Bayesian} sequential decision rule, we assigned priors $\pi_{1}$ and $\pi_{0}=1-\pi_{1}$ to hypotheses $H_{1}$ and $H_{0}$, respectively. The optimal rule can be summarized as:
\begin{gather*}
\pi_{1}\leq\pi_{L},\mbox{ stop and declare $H_{0}$},\\
\pi_{1}\geq\pi_{L}, \mbox{ stop and declare $H_{1}$},\\
\pi_{1}\in(\pi_{L},\pi_{U}),~\mbox{take one sample},\\
\mbox{Update } \pi_{j}:\pi_{j}(y)=\spc(H_{j}~\text{is true}/Y_{1}=y_{1}),\mbox{ and recurse.}
\end{gather*}
The corresponding stopping and decision rules are,
 \begin{equation}
 \phi_{n}(y_{1},\ldots,y_{n})=
 \begin{cases}
 0,& \text{if }\pi_{1}(y_{1},\ldots,y_{n})\in(\pi_{L},\pi_{U}),\\
 1,& \text{otherwise}.
 \end{cases}
 \end{equation}
 \begin{equation}
 \delta_{n}(y_{1},\ldots,y_{n})=
 \begin{cases}
 1,& \text{if }\pi_{1}(y_{1},\ldots,y_{n})\geq\pi_{U},\\
 0,& \pi_{1}(y_{1},\ldots,y_{n})\leq\pi_{L}.
 \end{cases}
 \end{equation}
 Note that it is generally not easy to compute $\pi_{L}$ and $\pi_{U}$. Implementing the optimal test, however, is easy. We do so by first rewriting the optimal Bayes sequential test in terms of the likelihood ratio. Later, we will generalize this idea to what are called Sequential Probability Ratio Tests. The posterior probability can be computed using Bayes' rule (assuming $\spc_{0}$ and $\spc_{1}$ have densities $p_{0}$ and $p_{1}$, respectively):
 \begin{align*}
 \pi_{1}(y_{1}\ldots,y_{n})&=\spc(H_{1}/Y_{1}=y_{1},\ldots,Y_{n}=y_{n})\\
 &=\frac{\spc(Y_{1}=y_{1},\ldots,Y_{n}=y_{n}/H_{1})\spc(H_{1})}{\sum_{i=0,1}\spc(Y_{1}=y_{1},\ldots,Y_{n}=y_{n}/H_{i})\spc(H_{i})}\\
 &=\frac{\pi_{1}\Pi_{k=1}^{n}p_{1}(y_{k})}{\sum_{i=0,1}\pi_{i}\Pi_{k=1}^{n}p_{i}(y_{k})}\\
 &=\frac{\pi_{1}L_{n}(y_{1},\ldots,y_{n})}{\pi_{1}L_{n}(y_{1},\ldots,y_{n})+\pi_{0}}.
 \end{align*}
 We can thus rewrite the optimal Bayes sequential test as
 \begin{equation}
 \phi_{n}(y_{1},\ldots,y_{n})=
 \begin{cases}
 0,& \text{if }L_{n}(y_{1},\ldots,y_{n})\in(\ubar{\pi},\bar{\pi}),\\
 1,& \text{otherwise},
 \end{cases}
 \end{equation}
 and
 \begin{equation}
 \delta_{n}(y_{1},\ldots,y_{n})=
 \begin{cases}
 1,& \text{if }L_{n}(y_{1},\ldots,y_{n})\geq\ubar{\pi},\\
 0,& L_{n}(y_{1},\ldots,y_{n})\leq\bar{\pi},
 \end{cases}
 \end{equation}
 where $\ubar{\pi}=\frac{\pi_{0}\pi_{L}}{\pi_{1}(1-\pi_{L})}$, and $\bar{\pi}\frac{\pi_{0}\pi_{U}}{\pi_{1}(1-\pi_{U})}$.
\section{Sequential Probability Ratio Test}
In general, one can define a family of sequential tests called \textit{Sequential Probability Ratio Tests} (also called SPRT, Wald's SPRT), with boundaries $a$ and $b$ $(a\leq1\leq b).$ In particular, for any two real numbers $a$ and $b$ satisfying $0 < a \leq 1 \leq b < \infty$, the SPRT with boundaries $a$ and $b$ (denoted by SPRT$( a,b )$), is defined as,
\begin{equation}
\phi_n(y_1,\ldots,y_n) =  
\begin{cases}
  0, & \text{if } a < \lambda(y_1,\ldots,y_n) < b\\
  1, & \text{otherwise}
\end{cases}
\end{equation}
and,
\begin{equation}
\delta_n(y_1,\ldots,y_n) =  
\begin{cases}
  1, & \text{if } \lambda(y_1,\ldots,y_n) \geq a\\
  0, & \text{if } \lambda(y_1,\ldots,y_n) \leq b.
\end{cases}
\end{equation}
Note that the rule is left arbitrary if $a=b$. Thus the SPRT($a,b$) continues sampling until the likelihood ratio $\lambda_n$ falls outside the ``boundaries" $a$ and $b$, and then chooses $H_1$ if $\lambda_n \geq b$ and $H_0$ if $\lambda_n \leq a$. \\\\
\begin{rem}
By convention, the likelihood ratio with no samples is taken to be one, \textit{i.e.,} $L_0 \equiv 1.$
\end{rem}
\begin{rem}
If $a=1<b$, then take no sample and output $H_0$.
\end{rem}
\begin{rem}
If $a<b=1$, then take no sample and output $H_1$.
\end{rem}
\begin{rem}
If $a=b=1$, then take no sample and choose $H_0$ or $H_1$ \textit{arbitrarily}.
\end{rem}
\begin{exmp}{Sequential detection of a constant signal}
We now consider the problem of detecting a constant signal $\theta$ in additive zero-mean Gaussian noise. We have
\begin{eqnarray*}
H_0: Y_k=N_k, & k=1,2,3,\ldots,\\
H_1: Y_k=N_k+\theta, & k=1,2,3,\ldots,
\end{eqnarray*}
where $\theta \in \mathbb{R}_+$ is a fixed number and $\{N_k\}_{k=1}^\infty$ is an i.i.d. sequence of noise samples distributed $\mathcal{N}(0,\sigma^2)$.
\par We can calculate the likelihood ratio for this case as follows,
\begin{flalign*}
L_n(\ubar{y}) = & \frac{p_1(\ubar{y})}{p_0(\ubar{y})}\\
=& \prod_{k=1}^{n}\frac{\frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(y_k-\theta)^2}{2\sigma^2}}}{\frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{y_k^2}{2\sigma^2}}}\\
=&\exp\Big\{\sum_{k=1}^{n}\frac{\theta(y_k-\frac{\theta}{2})}{\sigma^2}\Big\}.
\end{flalign*}
Therefore, SPRT($a,b$) would suggest sampling until
\begin{equation*}
\sum_{k=1}^{n}\frac{\theta(y_k-\frac{\theta}{2})}{\sigma^2} \notin (\log a, \log b),
\end{equation*}
and declare,
\begin{eqnarray*}
H_0: \text{ if } \log{L_n} \leq \log{a},\\
H_1: \text{ if } \log{L_n} \geq \log{b}.
\end{eqnarray*}
\end{exmp}
In addition to the optimality of SPRT $ (\ubar{\pi},\bar{\pi}) $ in the Bayesian setting, SPRT $ (a,b) $ also has an optimality property similar to that in the Neyman-Pearson type setting. To this end, let us first define some notations.
\par Let
\begin{eqnarray}
\mathbb{P}_F(\ubar{\phi},\ubar{\delta}) = \mathbb{P}_0[\delta_N(Y_1,\ldots,Y_N)=1],\\
\mathbb{P}_M(\ubar{\phi},\ubar{\delta}) = \mathbb{P}_1[\delta_N(Y_1,\ldots,Y_N)=0],
\end{eqnarray}
where,
\begin{eqnarray*}
N\equiv N(\ubar\phi)= &\text{Random time at which we stop.}\\
=& \min\{n\geq 0, \phi_n(Y_1,\ldots,Y_n)=1\}.
\end{eqnarray*}
$N(\ubar\phi)$ is also called the sample number of $\ubar\phi$. We then have the following result.
\begin{theorem}{(Wald-Wolfowitz Theorem `98):}
Suppose that $ (\ubar\phi^*,\ubar\delta^*) $ represent the SPRT$ (a,b) $ and $ (\ubar\phi,\ubar\delta) $ is any other sequential test for which,
\begin{eqnarray*}
\mathbb{P}_F(\ubar\phi,\ubar\delta)\leq \mathbb{P}_F(\ubar\phi^*,\ubar\delta^*), \\
\text{ and }\mathbb{P}_M(\ubar\phi,\ubar\delta) \leq \mathbb{P}_M(\ubar\phi^*,\ubar\delta^*)
\end{eqnarray*}
Then, $ \mathbb{E}_j[N(\ubar\phi)] \geq \mathbb{E}_j[N(\ubar\phi^*)] ~\forall j \in \{0,1\}. $
\end{theorem}
Thus we can see form the above result that for a given level of performance, no sequential decision rule has a smaller expected sample size than the SPRT with that performance. It should be noted here that a fixed sample size detector (no randomness in sample size as in the theorem) is a special case of the sequential decision rule. Hence the theorem asserts that the \textit{average} sample size of an SPRT is no larger than the sample size of a fixed sample size detector with the same performance. The theorem also implies that given expected sample sizes, no sequential decision rule has smaller error probabilities than the SPRT. 
\par As is clear from the theorem, SPRTs turn out to be optimal in terms of $\mathbb{P}_F, \mathbb{P}_M$ and the number of samples if optimized altogether. Hence the natural question to ask is how to set $a$ and $b$ in an SPRT to achieve a desired performance (\textit{i.e.,} $\mathbb{P}_F, \mathbb{P}_M$). This question is the topic of interest for the next section.
\subsection{Wald's Approximation}
Suppose $(\ubar{\phi},\ubar{\delta})\equiv SPRT(a,b),~a<1<b$ that achieves,
\begin{flalign}
\alpha = &  \mathbb{P}_F(\ubar{\phi},\ubar{\delta})\\
\gamma = & 1-\beta = \mathbb{P}_M(\ubar{\phi},\ubar{\delta}).
\end{flalign}
The rejection region of $(\ubar\phi,\ubar\delta)$ is by definition,
\begin{flalign}
\Gamma_1 = & \{y \in \mathbb{R}^\infty | L_N(y_1,\ldots,y_N)\geq b\},\nonumber\\
=& \bigcup\limits_{n=1}^{\infty}\{y \in \mathbb{R}^\infty |N=n, L_n(y_1,\ldots,y_n)\geq b\}.
\end{flalign}
Let $\{Q_n\} \triangleq \{y \in \mathbb{R}^\infty |N=n, L_n(y_1,\ldots,y_n)\geq b\}$. Clearly, $\{Q_n\}$ are mutually exclusive. We thus have,
\begin{flalign*}
\alpha = & \mathbb{P}_0(\Gamma_1)\\
=& \sum_{n=1}^{\infty} \mathbb{P}_0(Q_n)\\
=& \sum_{n=1}^{\infty} \int_{Q_n} \prod_{k=1}^{n}[p_0(y_k)d(y_k)]\\
\leq & \sum_{n=1}^{\infty} \int_{Q_n} \prod_{k=1}^{n}[p_1(y_k)d(y_k)]\times \frac{1}{b}.
\end{flalign*}
The last inequality follows from the fact that in $Q_n$ we have $\prod_{k=1}^{n}p_0(y_k)\leq \frac{1}{b}\times \prod_{k=1}^{n}p_1(y_k)$. Hence,
\begin{flalign}
\notag\alpha \leq & \frac{1}{b} \times \notag\mathbb{P}_1[L_N(Y_1,\ldots,Y_N) \geq b],\\
\notag=& \frac{1}{b}\times \mathbb{P}_1[\Gamma_1],\\
\notag=& \frac{1}{b} \times (1-\gamma),
\end{flalign}
i.e.,
\begin{equation}\label{eqn:rel1}
\alpha \leq \frac{1-\gamma}{b}. 
\end{equation}
Letting $\{A_n\} \triangleq \{y \in \mathbb{R}^\infty |N=n, L_n(y_1,\ldots,y_n)\leq a\}$, we have 
\begin{flalign*}
\gamma = & \mathbb{P}_1(\Tau_0),\\
=& \sum_{n=1}^{\infty} \mathbb{P}_1(A_n),\\
=& \sum_{n=1}^{\infty} \int_{A_n} \prod_{k=1}^{n}[p_1(y_k)d(y_k)],\\
\leq & \sum_{n=1}^{\infty} \int_{A_n} \prod_{k=1}^{n}[p_0(y_k)d(y_k)]\times a.
\end{flalign*}
The last inequality follows from the fact that in $A_n$ we have $\prod_{k=1}^{n}p_1(y_k)\leq a \times \prod_{k=1}^{n}p_0(y_k).$ Hence,
\begin{flalign*}
\notag\gamma \leq & a \times \mathbb{P}_0[L_N(Y_1,\ldots,Y_N) \leq b],\\
\notag=& a\times \mathbb{P}_0[\Tau_0],\\
\notag=& \frac{1}{b} \times (1-\alpha),
\end{flalign*}
i.e.,
\begin{equation}\label{eqn:rel2}
\gamma \leq \frac{(1-\alpha)}{b}
\end{equation}
Hence, from equations \eqref{eqn:rel1} and \eqref{eqn:rel2}, we have,
\begin{eqnarray}
b\leq \frac{1-\gamma}{\alpha}\\
\text{and, } a\geq \frac{\gamma}{1-\alpha}.
\end{eqnarray}
Note that these are inequalities, which mean that this is a conservative guideline to get $\mathbb{P}_F \leq \alpha$, and $\mathbb{P}_M \leq \gamma$, if one sets the corresponding $a$ and $b$ as given by equations (3) and (4). Moreover, if we assume that whenever $L_n(y_1,\ldots,y_n)$ crosses $a$ or $b$ and stops, the overshoot $(L_n-b)$ or $(a-L_n)$ is negligible, then we get 
\begin{equation}
b \approx \frac{1-\gamma}{\alpha} \text{and } a\approx \frac{\gamma}{1-\alpha}. 
\end{equation}
This is called \textit{Wald's approximation.}
\subsection{Calculation of expected number of samples in a SPRT}
\begin{lemma}[Wald's Lemma]
Let $Z_1, Z_2, \ldots$ be i.i.d. random variables with mean $ \mu $. Let $K\geq 0$ be any integer valued random variable with finite expectation, $ \mathbb{E}[K] < \infty $ and such that the event $ \{K=k\} $ is completely determined by $(Z_1,\ldots, Z_k)$ $\forall k$. Then,
\begin{equation}
\mathbb{E}\Big[\sum_{i=1}^{K}Z_i\Big] = [\mathbb{E}K]\mu.
\end{equation}
\end{lemma}
\begin{definition}[K-L divergence] Given two distributions $ P_1 $ and $ P_0 $ on $ \Gamma $, the K-L divergence between $ P_1 $ and $ P_0 $ is,
\begin{equation*}
D(P_1||P_0) = \mathbb{E}_1\Big[ \log{\frac{P_1(Y)}{P_0(Y)}}\Big].
\end{equation*}
\end{definition}
\par Recall that $N = \min\{n\geq 0, \phi_n(Y_1,\ldots,Y_n)=1\}.$ Clearly, the event $\{N=n\}$ just depends on $Y_1,\ldots, Y_n.$ Additionally, if we assume that $\mathbb{E}[N]<\infty$, then we can write
\begin{eqnarray}\label{eqn:f1}
\notag\mathbb{E}_0[\log L_N(Y_1,\ldots, Y_n)] = & \mathbb{E}_0\Big[\sum_{k=1}^{N}\log \frac{p_1(Y_k)}{p_0(Y_k)}\Big]\\
\notag=& \mathbb{E}_0[N].\mathbb{E}_0\Big[\log \frac{p_1(Y_k)}{p_0(Y_k)}\Big]\\
=& -\mathbb{E}_0[N].D[P_0||P_1].
\end{eqnarray}
Here the second equality follows from a direct consequence Wald's lemma on the function $g_k(Y_k)\triangleq \log\frac{p_1(Y_k)}{p_0(Y_k)}$ and the second inequality follows from the definition of K-L divergence.
\par Again, we can alternatively use the Wald's approximation which essentially says that at the stopping time either $L_n(Y_1,\ldots,Y_n) \approx a \text{ or } \approx b$. If we take the expectation of the likelihood over hypothesis $H_0$,
\begin{equation}\label{eqn:f2}
\mathbb{E}_0[\log L_n(Y_1,\ldots,Y_n)] \approx (\log a)(1-\alpha) + (\log b)\alpha.
\end{equation}
This is because under hypothesis $H_0$, the likelihood ratio can take the value $a$ with probability $(1-\alpha)$ which is essentially $\mathbb{P}_0(\Gamma_0)$, or value $b$ with probability $(\alpha)$ which is essentially $\mathbb{P}_0(\Gamma_1)$.
From \eqref{eqn:f1} and \eqref{eqn:f2} we can now write,
\begin{eqnarray}
\notag\mathbb{E}_0[N] \approx & \frac{(1-\alpha)\log a + \alpha\log b}{-D[P_0||P_1]},\\
=& \frac{(1-\alpha)\log \frac{1-\alpha}{\gamma} + \alpha\log \frac{\alpha}{1-\gamma}}{D[P_0||P_1]}
\end{eqnarray}
Similarly, we can repeat the exercise to calculate the expected number of samples under hypothesis $H_1$.
\begin{eqnarray}\label{eqn:g1}
\notag\mathbb{E}_1[\log L_N(Y_1,\ldots, Y_n)] = & \mathbb{E}_1\Big[\sum_{k=1}^{N}\log \frac{p_1(Y_k)}{p_0(Y_k)}\Big]\\
\notag=& \mathbb{E}_1[N].\mathbb{E}_1\Big[\log \frac{p_1(Y_k)}{p_0(Y_k)}\Big]\\
=& \mathbb{E}_1[N].D[P_1||P_0].
\end{eqnarray}
Here the second equality follows from a direct consequence Wald's lemma on the function $g_k(Y_k)\triangleq \log\frac{p_1(Y_k)}{p_0(Y_k)}$, and the second inequality follows from the definition of K-L divergence.
\par Again, we can alternatively use the Wald's approximation which essentially says that at the stopping time either $L_n(Y_1,\ldots,Y_n) \approx a \text{ or } \approx b$. If now  we take the expectation of the likelihood over hypothesis $H_1$,
\begin{equation}\label{eqn:g2}
\mathbb{E}_1[\log L_n(Y_1,\ldots,Y_n)] \approx (\log a)(\gamma) + (\log b)(1-\gamma).
\end{equation}
This is because under hypothesis $H_1$, the likelihood ratio can take the  value $a$ with probability $\gamma$ which is essentially $\mathbb{P}_1(\Tau_0)$, or value $b$ with probability $(1-\gamma)$ which is essentially $\mathbb{P}_1(\Tau_1)$.\\
From \eqref{eqn:g1} and \eqref{eqn:g2} we can now write,
\begin{eqnarray}
\mathbb{E}_1[N] \approx & \frac{(1-\gamma)\log \frac{1-\gamma}{\alpha} + \gamma\log \frac{\gamma}{1-\alpha}}{D[P_1||P_0]}.
\end{eqnarray}
\end{document}